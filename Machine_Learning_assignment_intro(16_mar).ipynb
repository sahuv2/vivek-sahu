{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how  can they be mitigated?"
      ],
      "metadata": {
        "id": "krMRRNYGkFYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overfitting happens when a model learns the training data too well, to the point where it starts to memorize noise or random.\n",
        "\n",
        "overfitting have low bias and high variance\n",
        "\n",
        "Mitigation of Overfitting:\n",
        "\n",
        "Regularization,\n",
        "Cross-validation,\n",
        "Feature selection,\n",
        "Data augmentation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Underfitting occurs when a machine learning model is too simple to capture the underlying patterns in the data. As a result, it performs poorly not only on the training data but also on unseen data.\n",
        "\n",
        "underfitting high bias and high variance\n",
        "\n",
        "Mitigation of Underfitting:\n",
        "\n",
        "More complex model,\n",
        "Feature engineering,\n",
        "Hyperparameter tuning\n",
        "\n"
      ],
      "metadata": {
        "id": "YdB3srgBkFT-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2: How can we reduce overfitting? Explain in brief.\n"
      ],
      "metadata": {
        "id": "ntTN2WRzkFM_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overfitting occurs when a machine learning model performs well on the training data but fails to generalize to new, unseen data. To reduce overfitting, you can employ several techniques:\n",
        "\n",
        "Increase the size of the training dataset: More data can help the model capture a wider range of patterns and variations, reducing the chances of overfitting.\n",
        "\n",
        "Use cross-validation: Cross-validation involves splitting the available data into multiple subsets. The model is trained on different combinations of these subsets, and their performance is evaluated. This technique helps assess the model's generalization ability and can identify overfitting."
      ],
      "metadata": {
        "id": "rffmrk4WkFJu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\n"
      ],
      "metadata": {
        "id": "34RvrLrHkFGu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Underfitting is a phenomenon that occurs when a machine learning model is unable to capture the underlying patterns and relationships in the training data, resulting in poor performance and low accuracy. It happens when the model is too simple or lacks the necessary complexity to adequately represent the data.\n",
        "\n",
        "Underfitting can occur in various scenarios in machine learning, including:\n",
        "\n",
        "Insufficient Model Complexity: If the model used is too simple or has a limited number of parameters, it may struggle to capture the complexity of the data. For example, using a linear regression model to fit a highly nonlinear relationship between the features and the target variable could lead to underfitting.\n",
        "\n",
        "Insufficient Training: When the training dataset is small or lacks diversity, the model may not have enough examples to learn the underlying patterns. This can result in underfitting as the model fails to generalize well to unseen data."
      ],
      "metadata": {
        "id": "7rtiGX9UkFDe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and  variance, and how do they affect model performance?"
      ],
      "metadata": {
        "id": "fz0LyhQtkFAu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bias-variance tradeoff is a fundamental concept in machine learning that relates to the performance of a predictive model. It refers to the balance between two types of errors that a model can make: bias and variance.\n",
        "\n",
        "bias is a sysmatic error and variance is a random error\n",
        "\n",
        "The goal is to find an optimal balance between bias and variance that minimizes the overall error of the model on unseen data"
      ],
      "metadata": {
        "id": "czJJsPKekE9-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.  How can you determine whether your model is overfitting or underfitting?\n"
      ],
      "metadata": {
        "id": "y7sXdFhDkE7F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overfitting and underfitting are common challenges in machine learning models. They occur when a model performs poorly on unseen data due to either excessive complexity or insufficient complexity, respectively. Fortunately, several methods can help detect these issues. Here are some common methods for detecting overfitting and underfitting:\n",
        "\n",
        "Training and Validation Curves: Plotting the model's performance on the training and validation datasets over multiple training iterations can provide insights. If the training error continues to decrease while the validation error plateaus or starts to increase, it suggests overfitting. Conversely, if both errors are high and relatively similar, it indicates underfitting.\n",
        "\n",
        "Cross-Validation: Cross-validation involves partitioning the data into multiple subsets, performing training and evaluation iteratively, and then aggregating the results. If the model performs significantly better on the training subsets than on the validation subsets, it may be overfitting."
      ],
      "metadata": {
        "id": "2D8PaxyukE4u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias  and high variance models, and how do they differ in terms of their performance?"
      ],
      "metadata": {
        "id": "_dKdCQa-kE2P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bias:\n",
        "Bias refers to the error introduced by approximating a real-world problem with a simplified model.\n",
        "\n",
        "Linear regression with few features and a simple linear relationship between the features and target variable.\n",
        "Logistic regression with a linear decision boundary for a non-linear classification problem.\n",
        "\n",
        "\n",
        "Variance, on the other hand, refers to the sensitivity of the model to the fluctuations in the training set\n",
        "\n",
        "Examples of high variance models:\n",
        "\n",
        "Decision trees with a large depth, which can create complex and deep branches.\n",
        "Neural networks with a large number of layers and parameters, leading to overfitting.\n",
        "\n",
        "Differences in performance:\n",
        "High bias models generally produce similar results across different training sets, as they are unable to capture the underlying patterns in the data. They tend to have a high training error and high test error, suggesting a systematic error in the model's predictions."
      ],
      "metadata": {
        "id": "vltYagKqoE1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe  some common regularization techniques and how they work."
      ],
      "metadata": {
        "id": "-6L7j7XloEyB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization in machine learning is a technique used to prevent overfitting, which occurs when a model becomes too complex and starts to fit the training data too closely, resulting in poor generalization to unseen data. Regularization helps to control the model's complexity by adding a penalty term to the loss function\n",
        "\n",
        "regulization is low bias and low variance"
      ],
      "metadata": {
        "id": "fJsDHg7zoEvw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVGhrQ3dj6pR"
      },
      "outputs": [],
      "source": []
    }
  ]
}